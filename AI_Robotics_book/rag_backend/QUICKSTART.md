# ğŸš€ Quick Start - RAG Search Setup

Your documentation now has **AI-powered search** with RAG (Retrieval Augmented Generation)!

## What You Get

âœ… **Floating chat removed** from home page  
âœ… **Smart search bar** in top-right navbar (press `Ctrl+K`)  
âœ… **RAG-powered answers** from your documentation  
âœ… **FictionLab-inspired dark theme** with orange accents  
âœ… **Tokenized document ingestion** to Qdrant vector database

---

## ğŸ¯ 3-Step Setup

### 1ï¸âƒ£ Install & Configure

```bash
cd rag_backend

# Install dependencies
pip install -r requirements.txt

# Create .env file
cp .env.example .env
```

**Edit `.env` with your keys:**

```env
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=
GEMINI_API_KEY=your-gemini-api-key
QDRANT_COLLECTION=ai_robotics_docs
```

### 2ï¸âƒ£ Start Qdrant (Vector Database)

**Option A - Docker (Recommended):**
```bash
docker run -p 6333:6333 qdrant/qdrant
```

**Option B - Qdrant Cloud:**
1. Sign up at https://cloud.qdrant.io
2. Create a cluster
3. Use cluster URL in `.env`

### 3ï¸âƒ£ Ingest & Run

```bash
# Tokenize and embed your documentation
python ingest.py

# Start RAG server
python server.py
```

In another terminal:
```bash
# Start docs site
cd ../docs
npm start
```

---

## âœ¨ Using the Search

1. **Open**: http://localhost:3000/AI-driven-development/
2. **Search**: Click search bar (top-right) or press `Ctrl+K`
3. **Ask**: Type any question about your robotics documentation
4. **Get Answer**: AI generates intelligent responses with sources!

---

## ğŸ”§ What Happens Behind the Scenes

### During Ingestion (`python ingest.py`)

1. **Reads** all `.md` files from `../docs/docs/`
2. **Tokenizes** text into 512-token chunks (50 overlap)
3. **Embeds** using FastEmbed (BAAI/bge-small-en model)
4. **Stores** vectors in Qdrant with metadata

```
ğŸ“„ 01-intro-to-ros2.md
   â†“ Split into chunks
   â†“ Create embeddings
   â†“ Store in Qdrant
âœ… 156 chunks indexed
```

### During Search

1. **User query** â†’ Embedded with same model
2. **Vector search** â†’ Find top-K similar chunks
3. **LLM generation** â†’ Google Gemini creates answer from context
4. **Return** â†’ Answer + source documents

---

## ğŸ“ File Structure

```
rag_backend/
â”œâ”€â”€ .env              # Your configuration (create this!)
â”œâ”€â”€ .env.example      # Template
â”œâ”€â”€ requirements.txt  # Python dependencies
â”œâ”€â”€ ingest.py         # Document tokenization & embedding
â”œâ”€â”€ rag_backend.py    # RAG logic with Google Gemini
â”œâ”€â”€ server.py         # FastAPI server
â”œâ”€â”€ setup_check.py    # Setup validation script
â”œâ”€â”€ QUICKSTART.md     # This file
â””â”€â”€ README.md         # Detailed documentation
```

---

## ğŸ§ª Test Your Setup

### Check Setup
```bash
python setup_check.py
```

### Test RAG Endpoint
```bash
curl -X POST http://localhost:8001/query \
  -H "Content-Type: application/json" \
  -d '{"query":"What is ROS 2?"}'
```

### Health Check
```bash
curl http://localhost:8001/health
```

---

## âš™ï¸ Configuration Options

### Adjust Chunk Size

Edit `ingest.py`:
```python
def chunk_text(text, chunk_size=512, chunk_overlap=50):
    # Larger chunks = more context, fewer chunks
    # Smaller chunks = more precise, more chunks
```

### Change Number of Results

Edit `.env`:
```env
TOP_K=4  # Number of document chunks to retrieve
```

### Use Different LLM

Edit `.env`:
```env
CHAT_MODEL=gpt-4  # or gpt-3.5-turbo, gpt-4-turbo, etc.
```

---

## ğŸ› Troubleshooting

| Problem | Solution |
|---------|----------|
| **"Could not connect to Qdrant"** | Start Qdrant: `docker run -p 6333:6333 qdrant/qdrant` |
| **"Gemini API error"** | Check GEMINI_API_KEY in `.env` |
| **"No results found"** | Run `python ingest.py` first |
| **"Module not found"** | Run `pip install -r requirements.txt` |
| **Search bar not visible** | Rebuild docs: `cd ../docs && npm run build` |

---

## ğŸ¨ Frontend Features

âœ… **Dark theme** - FictionLab-inspired design  
âœ… **Search bar** - Top-right navbar position  
âœ… **Keyboard shortcut** - Press `Ctrl+K` / `Cmd+K`  
âœ… **Dropdown results** - Appears below search  
âœ… **Loading states** - Visual feedback during search  
âœ… **Error handling** - Clear messages if backend is down  

---

## ğŸš€ Production Deployment

### Deploy RAG Backend

**Render / Railway / Fly.io:**
```bash
# Set environment variables
QDRANT_URL=https://your-qdrant-cluster.com
QDRANT_API_KEY=your-key
GEMINI_API_KEY=your-gemini-key
SERVER_PORT=8001
```

### Update Frontend

Edit `docs/docusaurus.config.js`:
```javascript
customFields: {
  ragApiUrl: 'https://your-rag-backend.com',
}
```

---

## ğŸ“š Additional Resources

- **Qdrant Docs**: https://qdrant.tech/documentation/
- **Google Gemini API**: https://ai.google.dev/gemini-api/docs
- **FastEmbed**: https://github.com/qdrant/fastembed
- **Docusaurus**: https://docusaurus.io

---

## ğŸ’¡ Next Steps

1. âœ… **Test search** - Try different queries
2. ğŸ“ **Add more docs** - Run `ingest.py` again after adding files
3. ğŸ¨ **Customize theme** - Edit `docs/src/css/custom.css`
4. ğŸš€ **Deploy** - Put your docs and RAG backend online

---

## â“ Need Help?

- Check [SETUP_GUIDE.md](SETUP_GUIDE.md) for detailed setup
- Review [README.md](README.md) for architecture details
- Open an issue on GitHub
- Check console logs for error messages

---

**Enjoy your AI-powered documentation! ğŸ‰**
