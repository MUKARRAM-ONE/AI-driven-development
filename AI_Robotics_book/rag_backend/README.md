# RAG Backend (Qdrant + Google Gemini)

This folder contains a minimal Retrieval-Augmented Generation (RAG) backend implementation that indexes your Docusaurus docs into Qdrant Cloud and exposes a simple search API that augments queries with retrieved context and generates answers via Google Gemini.

## Requirements

- A Qdrant Cloud instance (URL + API key)
- A Google Gemini API key

## Quickstart

1. Create a virtual environment and install dependencies:

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

2. Copy `.env.example` to `.env` and fill in your keys:

```bash
cp .env.example .env
# Edit .env and set GEMINI_API_KEY, QDRANT_URL, QDRANT_API_KEY
```

3. Ingest docs into Qdrant:

```bash
python ingest.py
```

4. Run the API server locally:

```bash
python server.py
```

5. POST queries to `http://localhost:8000/search` with JSON `{ "query": "How to ..." }`.

## Notes

- This is a minimal example to get you started. For production use you should add:
  - better text chunking (token-aware)
  - error handling and retries
  - rate limits and authentication for the API
  - vector index persistence and incremental updates
# RAG Search Backend

This directory is intended for the Retrieval-Augmented Generation (RAG) search backend for the textbook content.

## Conceptual Role

The RAG backend will enable users to query the textbook content using natural language and receive contextually relevant answers generated by an LLM. It will work by:
1.  **Indexing**: Processing all Markdown chapters from the `docs/` directory and storing them in a searchable format (e.g., vector database).
2.  **Retrieval**: When a user submits a query, it will retrieve the most relevant chunks of text from the indexed content.
3.  **Augmentation**: These retrieved chunks will be used as context to augment a prompt sent to a Large Language Model.
4.  **Generation**: The LLM will then generate a coherent and informative answer based on the user's query and the provided context.

## Implementation Details (High-Level)

-   **Framework**: Potentially LangChain, LlamaIndex, or a custom Python solution.
-   **Vector Database**: ChromaDB, FAISS, Pinecone, or similar.
-   **LLM Integration**: Google Gemini (default), Hugging Face models, or local models.
-   **API Endpoint**: A simple REST API (e.g., using FastAPI or Flask) to expose the search functionality.

## Placeholder File

-   `rag_backend.py`: A placeholder script to illustrate the entry point for the RAG backend. Actual implementation would be much more extensive.
