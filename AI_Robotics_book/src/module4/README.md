# Module 4 Code Examples

This directory contains Python code examples for Module 4: Vision-Language-Action (VLA).

## Files

-   `voice_command_transcriber.py`: A ROS 2 node that uses OpenAI Whisper to transcribe audio commands.
-   `cognitive_planner_node.py`: A ROS 2 node that uses an LLM to generate action sequences from text commands.
-   `action_executor_node.py`: A ROS 2 node that executes the action sequences generated by the LLM.

## How to Run

1.  Ensure you have a ROS 2 workspace (`ros2_ws`) and your `src/module4` examples are part of a ROS 2 package (e.g., `my_robot_vla_pkg`).
2.  Set your OpenAI API key as an environment variable: `export OPENAI_API_KEY="YOUR_API_KEY"`.
3.  Build your workspace: `colcon build`.
4.  Source your workspace: `source install/setup.bash`.

### Transcribing Voice Commands

```bash
ros2 run my_robot_vla_pkg voice_command_transcriber.py
```
*(You will need to speak into your microphone to provide commands.)*

### Cognitive Planning

```bash
# First, ensure the voice_command_transcriber is running in another terminal
ros2 run my_robot_vla_pkg cognitive_planner_node.py
```
*(This node will subscribe to transcribed text and publish action sequences.)*

### Executing Actions

```bash
# First, ensure cognitive_planner_node is running
ros2 run my_robot_vla_pkg action_executor_node.py
# This node will subscribe to action sequences and perform (simulated) robot actions.
```
