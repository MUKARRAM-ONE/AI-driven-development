"use strict";(globalThis.webpackChunkphysical_ai_robotics_textbook=globalThis.webpackChunkphysical_ai_robotics_textbook||[]).push([[818],{3080:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>u,frontMatter:()=>i,metadata:()=>l,toc:()=>p});var o=n(8168),a=(n(6540),n(5680));const i={id:"13-capstone-project",title:"Chapter 13: Capstone: Autonomous Humanoid Robot",sidebar_label:"13. Capstone: Autonomous Humanoid Robot"},r=void 0,l={unversionedId:"13-capstone-project",id:"13-capstone-project",title:"Chapter 13: Capstone: Autonomous Humanoid Robot",description:"Chapter 13 Autonomous Humanoid Robot",source:"@site/docs/13-capstone-project.md",sourceDirName:".",slug:"/13-capstone-project",permalink:"/AI-driven-development/docs/13-capstone-project",draft:!1,editUrl:"https://github.com/ai-driven-development/AI_Robotics_book/tree/main/docs/13-capstone-project.md",tags:[],version:"current",sidebarPosition:13,frontMatter:{id:"13-capstone-project",title:"Chapter 13: Capstone: Autonomous Humanoid Robot",sidebar_label:"13. Capstone: Autonomous Humanoid Robot"},sidebar:"tutorialSidebar",previous:{title:"12. Cognitive Planning with LLMs",permalink:"/AI-driven-development/docs/12-vla-llm-planning"}},s={},p=[{value:"Chapter 13: Capstone: Autonomous Humanoid Robot",id:"chapter-13-capstone-autonomous-humanoid-robot",level:2},{value:"13.1 Project Overview",id:"131-project-overview",level:3},{value:"13.2 System Architecture",id:"132-system-architecture",level:3},{value:"13.3 Implementation Steps",id:"133-implementation-steps",level:3},{value:"13.4 Testing and Demonstration",id:"134-testing-and-demonstration",level:3}],c={toc:p},m="wrapper";function u({components:e,...t}){return(0,a.yg)(m,(0,o.A)({},c,t,{components:e,mdxType:"MDXLayout"}),(0,a.yg)("h2",{id:"chapter-13-capstone-autonomous-humanoid-robot"},"Chapter 13: Capstone: Autonomous Humanoid Robot"),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Objective"),": Integrate all module concepts into a single, functional system."),(0,a.yg)("h3",{id:"131-project-overview"},"13.1 Project Overview"),(0,a.yg)("p",null,"The capstone project brings together all the knowledge and skills you've acquired throughout this textbook. Your goal is to create an ",(0,a.yg)("strong",{parentName:"p"},"Autonomous Humanoid Robot")," in a simulated environment that can:"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Receive a voice command"),": Using the VLA system (Whisper)."),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Plan a path"),": Using Nav2."),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Navigate obstacles"),": Using Nav2 and sensor data."),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Identify an object"),": Using Isaac ROS perception."),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Manipulate it"),": A simple pick-and-place operation.")),(0,a.yg)("p",null,"This project will demonstrate the power of integrating different robotic subsystems \u2013 sensing, perception, planning, control, and human interaction \u2013 into a cohesive, intelligent agent capable of performing complex tasks."),(0,a.yg)("h3",{id:"132-system-architecture"},"13.2 System Architecture"),(0,a.yg)("p",null,"The integrated system architecture for the capstone project combines components from all four modules. Here's a high-level overview of the data flow and interaction:"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Voice Input"),': A human user provides a voice command (e.g., "Robot, go to the kitchen and pick up the apple.").'),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Transcription (Module 4 - Whisper)"),": The Voice Command Transcriber node (Chapter 11) uses OpenAI Whisper to convert the audio into text."),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Cognitive Planning (Module 4 - LLM)"),": The Cognitive Planner node (Chapter 12) receives the transcribed text, queries an LLM with the current robot and environment context, and receives a structured sequence of ROS 2 actions (e.g., navigate, find, pick)."),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Action Execution (ROS 2 - Modules 1, 2, 3)"),": A dedicated Action Executor node parses the LLM's action sequence and translates it into specific ROS 2 commands:",(0,a.yg)("ul",{parentName:"li"},(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Navigation (Module 3 - Nav2)"),": If the action is ",(0,a.yg)("inlineCode",{parentName:"li"},"navigate_to_pose"),", the executor sends a goal to the Nav2 action server (Chapter 10). Nav2 uses maps and sensor data (from Isaac Sim/ROS) to guide the robot."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Perception (Module 3 - Isaac ROS)"),": If the action is ",(0,a.yg)("inlineCode",{parentName:"li"},"find_object"),", the executor triggers Isaac ROS perception nodes (Chapter 9) to identify the object using simulated camera/depth data from Isaac Sim (Chapter 8)."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Manipulation (Module 1 - ROS 2 control)"),": If the action is ",(0,a.yg)("inlineCode",{parentName:"li"},"pick_up_object"),", the executor sends joint commands (Chapter 3) to the robot's end-effector controllers (defined in URDF, Chapter 4), possibly via a ",(0,a.yg)("inlineCode",{parentName:"li"},"JointTrajectoryController"),"."))),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Simulation & Visualization (Module 2 - Gazebo/Unity)"),": The robot performs these actions in Isaac Sim (for physics and sensors, Chapter 8), or Gazebo for general simulation (Chapter 5), with its state streamed to Unity for high-fidelity visualization (Chapter 6).")),(0,a.yg)("h3",{id:"133-implementation-steps"},"13.3 Implementation Steps"),(0,a.yg)("p",null,"Here's a step-by-step guide to assembling your autonomous humanoid:"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Setup Integrated Environment"),": Ensure your Isaac Sim, ROS 2, and all relevant Isaac ROS Docker containers are correctly configured and communicating."),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Assemble VLA Pipeline"),":",(0,a.yg)("ul",{parentName:"li"},(0,a.yg)("li",{parentName:"ul"},"Launch your ",(0,a.yg)("inlineCode",{parentName:"li"},"VoiceCommandTranscriber")," node."),(0,a.yg)("li",{parentName:"ul"},"Launch your ",(0,a.yg)("inlineCode",{parentName:"li"},"CognitivePlanner")," node, ensuring it can access your LLM API."),(0,a.yg)("li",{parentName:"ul"},"Implement a basic ",(0,a.yg)("inlineCode",{parentName:"li"},"ActionExecutor")," node that subscribes to ",(0,a.yg)("inlineCode",{parentName:"li"},"/robot_action_sequence")," and can call Nav2 goals, perception services, and joint control topics."))),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Connect Planner to Nav2"),": Configure your ",(0,a.yg)("inlineCode",{parentName:"li"},"ActionExecutor")," to properly send ",(0,a.yg)("inlineCode",{parentName:"li"},"NavigateToPose")," goals to the Nav2 stack, which should be running and configured for your humanoid robot."),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Integrate Perception for Object Identification"),": Implement a simple object detection node (using Isaac ROS or a simplified placeholder) that the ",(0,a.yg)("inlineCode",{parentName:"li"},"ActionExecutor")," can query (e.g., via a service) when a ",(0,a.yg)("inlineCode",{parentName:"li"},"find_object")," command is issued."),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Implement Manipulation Action"),": Develop basic joint commands or a simple manipulation sequence (e.g., closing grippers) that the ",(0,a.yg)("inlineCode",{parentName:"li"},"ActionExecutor")," can trigger for ",(0,a.yg)("inlineCode",{parentName:"li"},"pick_up_object"),"."),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Develop Robot Model (URDF/SDF)"),": Ensure your humanoid robot model in Isaac Sim has defined joints, sensors, and an end-effector suitable for manipulation.")),(0,a.yg)("h3",{id:"134-testing-and-demonstration"},"13.4 Testing and Demonstration"),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"End-to-End Test Scenario"),":"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},'Launch Isaac Sim with your humanoid robot in an environment with known objects (e.g., "apple").'),(0,a.yg)("li",{parentName:"ol"},"Launch all your ROS 2 nodes: ",(0,a.yg)("inlineCode",{parentName:"li"},"VoiceCommandTranscriber"),", ",(0,a.yg)("inlineCode",{parentName:"li"},"CognitivePlanner"),", ",(0,a.yg)("inlineCode",{parentName:"li"},"ActionExecutor"),", and Nav2 stack components."),(0,a.yg)("li",{parentName:"ol"},'Provide a voice command: "Robot, go to the table and pick up the apple."'),(0,a.yg)("li",{parentName:"ol"},"Observe the robot:",(0,a.yg)("ul",{parentName:"li"},(0,a.yg)("li",{parentName:"ul"},"Does it transcribe the command correctly?"),(0,a.yg)("li",{parentName:"ul"},"Does the LLM generate a valid action sequence?"),(0,a.yg)("li",{parentName:"ul"},"Does the robot navigate to the table?"),(0,a.yg)("li",{parentName:"ul"},"Does it identify the apple?"),(0,a.yg)("li",{parentName:"ul"},"Does it attempt to pick it up?"))),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Debugging"),": Use ROS 2 CLI tools (",(0,a.yg)("inlineCode",{parentName:"li"},"ros2 topic echo"),", ",(0,a.yg)("inlineCode",{parentName:"li"},"ros2 node info"),", ",(0,a.yg)("inlineCode",{parentName:"li"},"ros2 param get"),") and Isaac Sim's inspection tools to debug issues.")),(0,a.yg)("p",null,"This capstone project culminates your journey into Physical AI and Humanoid Robotics, preparing you to tackle real-world challenges in this exciting field."))}u.isMDXComponent=!0},5680:(e,t,n)=>{n.d(t,{xA:()=>c,yg:()=>d});var o=n(6540);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter(function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable})),n.push.apply(n,o)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach(function(t){a(e,t,n[t])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach(function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))})}return e}function l(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},i=Object.keys(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=o.createContext({}),p=function(e){var t=o.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},c=function(e){var t=p(e.components);return o.createElement(s.Provider,{value:t},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},g=o.forwardRef(function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=p(n),g=a,d=m["".concat(s,".").concat(g)]||m[g]||u[g]||i;return n?o.createElement(d,r(r({ref:t},c),{},{components:n})):o.createElement(d,r({ref:t},c))});function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,r=new Array(i);r[0]=g;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[m]="string"==typeof e?e:a,r[1]=l;for(var p=2;p<i;p++)r[p]=n[p];return o.createElement.apply(null,r)}return o.createElement.apply(null,n)}g.displayName="MDXCreateElement"}}]);