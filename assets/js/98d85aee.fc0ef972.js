"use strict";(globalThis.webpackChunkphysical_ai_robotics_textbook=globalThis.webpackChunkphysical_ai_robotics_textbook||[]).push([[624],{184:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/how_seech_recognation_system-be233e3edf6eaea00fd5c040ecf259cf.webp"},259:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/speech_rocgnation-deef8e2f91bff7caf1f1189f333328e8.png"},4517:(e,n,i)=>{i.d(n,{A:()=>t});const t="data:image/svg+xml;base64,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"},5529:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>s,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>l,toc:()=>g});var t=i(8168),o=(i(6540),i(5680));const r={id:"11-vla-whisper",title:"Chapter 11: Voice-to-Action with Whisper",sidebar_label:"11. Voice-to-Action with Whisper"},a=void 0,l={unversionedId:"11-vla-whisper",id:"11-vla-whisper",title:"Chapter 11: Voice-to-Action with Whisper",description:"Chapter 11: Voice-to-Action with Whisper",source:"@site/docs/11-vla-whisper.md",sourceDirName:".",slug:"/11-vla-whisper",permalink:"/AI-driven-development/docs/11-vla-whisper",draft:!1,editUrl:"https://github.com/ai-driven-development/AI_Robotics_book/tree/main/docs/11-vla-whisper.md",tags:[],version:"current",sidebarPosition:11,frontMatter:{id:"11-vla-whisper",title:"Chapter 11: Voice-to-Action with Whisper",sidebar_label:"11. Voice-to-Action with Whisper"},sidebar:"tutorialSidebar",previous:{title:"10. Nav2 Path Planning",permalink:"/AI-driven-development/docs/10-nav2-path-planning"},next:{title:"12. Cognitive Planning with LLMs",permalink:"/AI-driven-development/docs/12-vla-llm-planning"}},s={},g=[{value:"Chapter 11: Voice-to-Action with Whisper",id:"chapter-11-voice-to-action-with-whisper",level:2},{value:"11.1 Introduction to ASR (Automatic Speech Recognition)",id:"111-introduction-to-asr-automatic-speech-recognition",level:3},{value:"11.2 Using OpenAI Whisper",id:"112-using-openai-whisper",level:3},{value:"Setting up the Whisper API or Local Model",id:"setting-up-the-whisper-api-or-local-model",level:4},{value:"11.3 Creating a ROS 2 Node for Voice Command Transcription",id:"113-creating-a-ros-2-node-for-voice-command-transcription",level:3},{value:"11.4 Command Parsing",id:"114-command-parsing",level:3}],c={toc:g},p="wrapper";function d({components:e,...n}){return(0,o.yg)(p,(0,t.A)({},c,n,{components:e,mdxType:"MDXLayout"}),(0,o.yg)("h2",{id:"chapter-11-voice-to-action-with-whisper"},"Chapter 11: Voice-to-Action with Whisper"),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Objective"),": Convert spoken language into actionable text commands for robotic systems."),(0,o.yg)("p",null,(0,o.yg)("img",{alt:"VLA Architecture",src:i(4517).A,width:"800",height:"400"})),(0,o.yg)("h3",{id:"111-introduction-to-asr-automatic-speech-recognition"},"11.1 Introduction to ASR (Automatic Speech Recognition)"),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Automatic Speech Recognition (ASR)")," is the technology that allows computers to understand spoken language. It converts audio signals into text. ASR is a critical component for natural human-robot interaction, enabling robots to respond to voice commands, participate in conversations, and understand their environment through auditory cues."),(0,o.yg)("p",null,(0,o.yg)("img",{alt:"Speech Recognition System",src:i(259).A,width:"720",height:"540"}),"\n",(0,o.yg)("img",{alt:"How Speech Recognition Works",src:i(184).A,width:"2240",height:"1260"})),(0,o.yg)("p",null,"The ASR pipeline typically involves:"),(0,o.yg)("ol",null,(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("strong",{parentName:"li"},"Audio Capture"),": Recording sound from a microphone."),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("strong",{parentName:"li"},"Feature Extraction"),": Converting raw audio into meaningful features (e.g., spectrograms)."),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("strong",{parentName:"li"},"Acoustic Model"),": Mapping acoustic features to phonemes or sub-word units."),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("strong",{parentName:"li"},"Language Model"),": Predicting the sequence of words."),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("strong",{parentName:"li"},"Decoding"),": Generating the most probable sequence of words from the acoustic and language models.")),(0,o.yg)("h3",{id:"112-using-openai-whisper"},"11.2 Using OpenAI Whisper"),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"OpenAI Whisper")," is a powerful, general-purpose ASR model that has revolutionized speech-to-text transcription. It was trained on a massive dataset of diverse audio and text from the internet, making it highly robust to accents, background noise, and technical language. Crucially, Whisper is multilingual and can also translate speech from many languages into English."),(0,o.yg)("p",null,"For robotic applications, Whisper offers several advantages:"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"High Accuracy"),": Crucial for correctly interpreting commands."),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Multilingual Support"),": Enables interaction with a global user base."),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Open-Source & API"),": Flexible deployment options.")),(0,o.yg)("h4",{id:"setting-up-the-whisper-api-or-local-model"},"Setting up the Whisper API or Local Model"),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"1. Cloud-Based API (Recommended for simplicity and performance):")),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Obtain an API key from OpenAI."),(0,o.yg)("li",{parentName:"ul"},"Use the OpenAI Python client library to send audio files or streams to the Whisper API endpoint.")),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"2. Local Model (More computationally intensive):")),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Install the Whisper Python package (",(0,o.yg)("inlineCode",{parentName:"li"},"pip install openai-whisper"),")."),(0,o.yg)("li",{parentName:"ul"},"Download a pre-trained Whisper model (e.g., ",(0,o.yg)("inlineCode",{parentName:"li"},"base"),", ",(0,o.yg)("inlineCode",{parentName:"li"},"small"),", ",(0,o.yg)("inlineCode",{parentName:"li"},"medium"),", ",(0,o.yg)("inlineCode",{parentName:"li"},"large"),")."),(0,o.yg)("li",{parentName:"ul"},"Run the transcription locally using your GPU (if available).")),(0,o.yg)("h3",{id:"113-creating-a-ros-2-node-for-voice-command-transcription"},"11.3 Creating a ROS 2 Node for Voice Command Transcription"),(0,o.yg)("p",null,"We will create a ROS 2 Python node that:"),(0,o.yg)("ol",null,(0,o.yg)("li",{parentName:"ol"},"Captures audio from a microphone."),(0,o.yg)("li",{parentName:"ol"},"Sends the audio data to the OpenAI Whisper API."),(0,o.yg)("li",{parentName:"ol"},"Publishes the transcribed text to a ROS 2 topic.")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport pyaudio\nimport wave\nimport openai\nimport os\nimport io\n\nclass VoiceCommandTranscriber(Node):\n    def __init__(self):\n        super().__init__('voice_command_transcriber')\n        self.publisher_ = self.create_publisher(String, '/voice_command/text', 10)\n        self.get_logger().info('Voice Command Transcriber node started.')\n\n        # Initialize OpenAI API client (replace with your API key)\n        openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"YOUR_OPENAI_API_KEY\")\n\n        # Audio recording parameters\n        self.FORMAT = pyaudio.paInt16\n        self.CHANNELS = 1\n        self.RATE = 16000 # Sample rate for Whisper\n        self.CHUNK = 1024 # Buffer size\n        self.RECORD_SECONDS = 5 # Record for 5 seconds per command\n\n        self.audio = pyaudio.PyAudio()\n        self.stream = None # Will be opened when recording starts\n\n        self.get_logger().info(\"Ready to record voice commands.\")\n\n    def record_and_transcribe(self):\n        self.get_logger().info(\"Recording...\")\n        frames = []\n        self.stream = self.audio.open(format=self.FORMAT,\n                                      channels=self.CHANNELS,\n                                      rate=self.RATE,\n                                      input=True,\n                                      frames_per_buffer=self.CHUNK)\n        \n        for _ in range(0, int(self.RATE / self.CHUNK * self.RECORD_SECONDS)):\n            data = self.stream.read(self.CHUNK)\n            frames.append(data)\n        \n        self.get_logger().info(\"Finished recording. Transcribing...\")\n        \n        self.stream.stop_stream()\n        self.stream.close()\n\n        # Save audio to a BytesIO object to avoid disk I/O\n        audio_file = io.BytesIO()\n        wf = wave.open(audio_file, 'wb')\n        wf.setnchannels(self.CHANNELS)\n        wf.setsampwidth(self.audio.get_sample_size(self.FORMAT))\n        wf.setframerate(self.RATE)\n        wf.writeframes(b''.join(frames))\n        wf.close()\n        \n        audio_file.name = \"voice_command.wav\" # Required by OpenAI API\n        audio_file.seek(0) # Reset stream position\n\n        try:\n            response = openai.Audio.transcribe(\"whisper-1\", audio_file)\n            transcribed_text = response['text']\n            self.get_logger().info(f\"Transcribed: '{transcribed_text}'\")\n\n            msg = String()\n            msg.data = transcribed_text\n            self.publisher_.publish(msg)\n            self.get_logger().info(f\"Published voice command: '{transcribed_text}'\")\n        except Exception as e:\n            self.get_logger().error(f\"Error transcribing audio: {e}\")\n        \n        self.stream = self.audio.open(format=self.FORMAT,\n                                      channels=self.CHANNELS,\n                                      rate=self.RATE,\n                                      input=True,\n                                      frames_per_buffer=self.CHUNK)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    transcriber_node = VoiceCommandTranscriber()\n    # In a real application, you'd trigger record_and_transcribe based on a button press or wake word\n    # For now, we'll just run it once for demonstration.\n    transcriber_node.record_and_transcribe()\n    rclpy.spin_once(transcriber_node, timeout_sec=0.1) # Process pending messages\n    transcriber_node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n")),(0,o.yg)("h3",{id:"114-command-parsing"},"11.4 Command Parsing"),(0,o.yg)("p",null,"Once you have the transcribed text, the next step is to ",(0,o.yg)("strong",{parentName:"p"},"parse")," it into a format that the robot can understand. This often involves:"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Keyword Extraction"),': Identifying key verbs (e.g., "go", "pick", "find") and nouns (e.g., "kitchen", "red block").'),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Entity Recognition"),": Identifying specific objects, locations, or commands."),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Intent Classification"),": Determining the high-level goal of the command.")),(0,o.yg)("p",null,"This parsed command will then be fed into a cognitive planning system (like an LLM) to generate a sequence of robot actions."))}d.isMDXComponent=!0},5680:(e,n,i)=>{i.d(n,{xA:()=>c,yg:()=>m});var t=i(6540);function o(e,n,i){return n in e?Object.defineProperty(e,n,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[n]=i,e}function r(e,n){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),i.push.apply(i,t)}return i}function a(e){for(var n=1;n<arguments.length;n++){var i=null!=arguments[n]?arguments[n]:{};n%2?r(Object(i),!0).forEach(function(n){o(e,n,i[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):r(Object(i)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(i,n))})}return e}function l(e,n){if(null==e)return{};var i,t,o=function(e,n){if(null==e)return{};var i,t,o={},r=Object.keys(e);for(t=0;t<r.length;t++)i=r[t],n.indexOf(i)>=0||(o[i]=e[i]);return o}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)i=r[t],n.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(o[i]=e[i])}return o}var s=t.createContext({}),g=function(e){var n=t.useContext(s),i=n;return e&&(i="function"==typeof e?e(n):a(a({},n),e)),i},c=function(e){var n=g(e.components);return t.createElement(s.Provider,{value:n},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},I=t.forwardRef(function(e,n){var i=e.components,o=e.mdxType,r=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),p=g(i),I=o,m=p["".concat(s,".").concat(I)]||p[I]||d[I]||r;return i?t.createElement(m,a(a({ref:n},c),{},{components:i})):t.createElement(m,a({ref:n},c))});function m(e,n){var i=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var r=i.length,a=new Array(r);a[0]=I;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[p]="string"==typeof e?e:o,a[1]=l;for(var g=2;g<r;g++)a[g]=i[g];return t.createElement.apply(null,a)}return t.createElement.apply(null,i)}I.displayName="MDXCreateElement"}}]);