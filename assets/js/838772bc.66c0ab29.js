"use strict";(globalThis.webpackChunkphysical_ai_robotics_textbook=globalThis.webpackChunkphysical_ai_robotics_textbook||[]).push([[53],{5680:(e,n,t)=>{t.d(n,{xA:()=>g,yg:()=>d});var i=t(6540);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,i)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach(function(n){o(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function l(e,n){if(null==e)return{};var t,i,o=function(e,n){if(null==e)return{};var t,i,o={},a=Object.keys(e);for(i=0;i<a.length;i++)t=a[i],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)t=a[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var s=i.createContext({}),c=function(e){var n=i.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},g=function(e){var n=c(e.components);return i.createElement(s.Provider,{value:n},e.children)},p="mdxType",I={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},m=i.forwardRef(function(e,n){var t=e.components,o=e.mdxType,a=e.originalType,s=e.parentName,g=l(e,["components","mdxType","originalType","parentName"]),p=c(t),m=o,d=p["".concat(s,".").concat(m)]||p[m]||I[m]||a;return t?i.createElement(d,r(r({ref:n},g),{},{components:t})):i.createElement(d,r({ref:n},g))});function d(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var a=t.length,r=new Array(a);r[0]=m;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[p]="string"==typeof e?e:o,r[1]=l;for(var c=2;c<a;c++)r[c]=t[c];return i.createElement.apply(null,r)}return i.createElement.apply(null,t)}m.displayName="MDXCreateElement"},7091:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>r,default:()=>I,frontMatter:()=>a,metadata:()=>l,toc:()=>c});var i=t(8168),o=(t(6540),t(5680));const a={id:"12-vla-llm-planning",title:"Chapter 12: Cognitive Planning with LLMs",sidebar_label:"12. Cognitive Planning with LLMs"},r=void 0,l={unversionedId:"12-vla-llm-planning",id:"12-vla-llm-planning",title:"Chapter 12: Cognitive Planning with LLMs",description:"Chapter 12: Cognitive Planning with LLMs",source:"@site/docs/12-vla-llm-planning.md",sourceDirName:".",slug:"/12-vla-llm-planning",permalink:"/AI-driven-development/docs/12-vla-llm-planning",draft:!1,editUrl:"https://github.com/ai-driven-development/AI_Robotics_book/tree/main/docs/12-vla-llm-planning.md",tags:[],version:"current",sidebarPosition:12,frontMatter:{id:"12-vla-llm-planning",title:"Chapter 12: Cognitive Planning with LLMs",sidebar_label:"12. Cognitive Planning with LLMs"},sidebar:"tutorialSidebar",previous:{title:"11. Voice-to-Action with Whisper",permalink:"/AI-driven-development/docs/11-vla-whisper"},next:{title:"13. Capstone: Autonomous Humanoid Robot",permalink:"/AI-driven-development/docs/13-capstone-project"}},s={},c=[{value:"Chapter 12: Cognitive Planning with LLMs",id:"chapter-12-cognitive-planning-with-llms",level:2},{value:"12.1 Introduction to LLMs in Robotics",id:"121-introduction-to-llms-in-robotics",level:3},{value:"12.2 Prompt Engineering for Robot Action Sequences",id:"122-prompt-engineering-for-robot-action-sequences",level:3},{value:"Key Principles for Robotic Prompts",id:"key-principles-for-robotic-prompts",level:4},{value:"Example Prompt Structure",id:"example-prompt-structure",level:4},{value:"12.3 Integrating with an LLM API",id:"123-integrating-with-an-llm-api",level:3},{value:"12.4 Translating LLM Output to ROS 2 Actions",id:"124-translating-llm-output-to-ros-2-actions",level:3}],g={toc:c},p="wrapper";function I({components:e,...n}){return(0,o.yg)(p,(0,i.A)({},g,n,{components:e,mdxType:"MDXLayout"}),(0,o.yg)("h2",{id:"chapter-12-cognitive-planning-with-llms"},"Chapter 12: Cognitive Planning with LLMs"),(0,o.yg)("p",null,(0,o.yg)("strong",{parentName:"p"},"Objective"),": Use a Large Language Model to translate a high-level goal into a sequence of robot actions."),(0,o.yg)("p",null,(0,o.yg)("img",{alt:"VLA Sequence Diagram",src:t(9159).A,width:"800",height:"600"}),"\n",(0,o.yg)("img",{alt:"VLA Architecture",src:t(8711).A,width:"1250",height:"463"}),"\n","![VLA Flow]","(/img/VLA Arch.webp)\n",(0,o.yg)("img",{alt:"Open VLA",src:t(9960).A,width:"1240",height:"652"})),(0,o.yg)("h3",{id:"121-introduction-to-llms-in-robotics"},"12.1 Introduction to LLMs in Robotics"),(0,o.yg)("p",null,'Large Language Models (LLMs) have emerged as powerful tools for understanding and generating human language. Their ability to comprehend complex instructions, infer intent, and generate coherent text makes them incredibly valuable for extending the cognitive capabilities of robots. In the context of robotics, LLMs can act as a "high-level brain," translating abstract human commands into concrete, executable sequences of robot actions.'),(0,o.yg)("p",null,"Key benefits of LLMs in robotics:"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Natural Language Interface"),": Enables humans to interact with robots using everyday language."),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Cognitive Planning"),": Breaking down complex tasks into simpler sub-tasks."),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Context Understanding"),": Incorporating environmental and task context to refine plans."),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Error Recovery"),": Suggesting alternative actions or seeking clarification during execution failures.")),(0,o.yg)("h3",{id:"122-prompt-engineering-for-robot-action-sequences"},"12.2 Prompt Engineering for Robot Action Sequences"),(0,o.yg)("p",null,"The quality of an LLM's output is highly dependent on the ",(0,o.yg)("strong",{parentName:"p"},"prompt")," you provide. ",(0,o.yg)("strong",{parentName:"p"},"Prompt engineering")," is the art and science of crafting effective prompts to guide the LLM towards generating desired outputs. For robotics, this means designing prompts that elicit structured, executable action sequences."),(0,o.yg)("h4",{id:"key-principles-for-robotic-prompts"},"Key Principles for Robotic Prompts"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Clear Instructions"),': Explicitly state the desired output format (e.g., "Respond only with a JSON array of ROS 2 actions.").'),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Role-Playing"),': Assign the LLM a role (e.g., "You are a robot task planner.").'),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Few-Shot Examples"),": Provide examples of input commands and their corresponding valid action sequences."),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Constraints"),": Specify rules for valid actions, available tools, and environmental limitations."),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"Safety Guidelines"),": Include instructions to prioritize safety and avoid dangerous actions.")),(0,o.yg)("h4",{id:"example-prompt-structure"},"Example Prompt Structure"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},'"You are a robot task planner. Your goal is to convert natural language commands into a sequence of ROS 2 actions.\nAvailable ROS 2 Actions:\n- navigate_to_pose(x, y, yaw_degrees)\n- pick_up_object(object_name)\n- say_phrase(phrase)\n\nRespond ONLY with a JSON array where each object is an action.\n\nExample:\nUser: Go to the kitchen and pick up the apple.\nResponse:\n[\n  {"action": "navigate_to_pose", "params": {"x": 5.0, "y": 2.0, "yaw_degrees": 90.0}},\n  {"action": "pick_up_object", "params": {"object_name": "apple"}}\n]\n\nUser: {natural_language_command_from_whisper}\nResponse:\n')),(0,o.yg)("h3",{id:"123-integrating-with-an-llm-api"},"12.3 Integrating with an LLM API"),(0,o.yg)("p",null,"Similar to Whisper, you can integrate with an LLM API (e.g., OpenAI's GPT models) via their Python client library."),(0,o.yg)("p",null,"We'll create a ROS 2 node that:"),(0,o.yg)("ol",null,(0,o.yg)("li",{parentName:"ol"},"Subscribes to the transcribed text topic (e.g., ",(0,o.yg)("inlineCode",{parentName:"li"},"/voice_command/text"),")."),(0,o.yg)("li",{parentName:"ol"},"Constructs a prompt based on the received command and robot context."),(0,o.yg)("li",{parentName:"ol"},"Sends the prompt to the LLM API."),(0,o.yg)("li",{parentName:"ol"},"Parses the LLM's JSON response into a sequence of ROS 2 actions."),(0,o.yg)("li",{parentName:"ol"},"Publishes these actions to a new ROS 2 topic or directly calls action clients.")),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport openai\nimport os\nimport json\n\n# This conceptual node subscribes to transcribed voice commands and uses an LLM\n# to generate a sequence of ROS 2 actions.\n\nclass CognitivePlannerNode(Node):\n    def __init__(self):\n        super().__init__(\'cognitive_planner\')\n        self.subscription = self.create_subscription(\n            String,\n            \'/voice_command/text\',\n            self.command_callback,\n            10\n        )\n        self.action_sequence_publisher = self.create_publisher(String, \'/robot_action_sequence\', 10)\n        self.get_logger().info(\'Cognitive Planner node started.\')\n\n        openai.api_key = os.getenv("OPENAI_API_KEY", "YOUR_OPENAI_API_KEY") # Ensure API key is set\n\n        self.robot_context = {\n            "current_location": "living_room",\n            "available_objects": ["apple", "cup", "book"],\n            "known_locations": {"kitchen": (5.0, 2.0, 90.0), "bedroom": (-3.0, 4.0, 0.0)}\n        }\n\n    def command_callback(self, msg):\n        command_text = msg.data\n        self.get_logger().info(f"Received voice command: \'{command_text}\'")\n        \n        # Construct prompt for the LLM\n        prompt = self.create_llm_prompt(command_text)\n\n        try:\n            response = openai.ChatCompletion.create(\n                model="gpt-3.5-turbo", # or gpt-4\n                messages=[\n                    {"role": "system", "content": "You are a robot task planner. Convert commands into ROS 2 actions."},\n                    {"role": "user", "content": prompt}\n                ],\n                max_tokens=200,\n                temperature=0.0 # Make it deterministic\n            )\n            llm_response_content = response.choices[0].message.content.strip()\n            self.get_logger().info(f"LLM Response: {llm_response_content}")\n\n            action_sequence = json.loads(llm_response_content)\n            \n            # Publish the action sequence for execution\n            action_msg = String()\n            action_msg.data = json.dumps(action_sequence)\n            self.action_sequence_publisher.publish(action_msg)\n            self.get_logger().info("Published ROS 2 action sequence.")\n\n        except json.JSONDecodeError as e:\n            self.get_logger().error(f"LLM response not valid JSON: {e}")\n            self.get_logger().error(f"Raw LLM response: {llm_response_content}")\n        except Exception as e:\n            self.get_logger().error(f"Error calling LLM API: {e}")\n\n    def create_llm_prompt(self, command_text):\n        # This is where prompt engineering happens\n        # It should include current robot context and available actions\n        prompt = f"""You are a robot task planner. Your goal is to convert natural language commands into a sequence of ROS 2 actions.\nAvailable ROS 2 Actions:\n- navigate_to_pose(x, y, yaw_degrees): Navigate to a 2D pose (x,y) with a final orientation (yaw_degrees).\n- pick_up_object(object_name): Pick up a specified object.\n- find_object(object_name): Find a specified object using perception.\n- say_phrase(phrase): Make the robot speak a phrase.\n\nCurrent robot location: {self.robot_context[\'current_location\']}\nKnown locations: {json.dumps(self.robot_context[\'known_locations\'])}\nAvailable objects: {json.dumps(self.robot_context[\'available_objects\'])}\n\nRespond ONLY with a JSON array where each object is an action. Do not include any other text.\nIf a location is mentioned, use its coordinates from \'known_locations\'.\nIf you cannot fulfill the command with available actions, return an empty array.\n\nUser: {command_text}\nResponse:\n"""\n        return prompt\n\ndef main(args=None):\n    rclpy.init(args=args)\n    cognitive_planner_node = CognitivePlannerNode()\n    rclpy.spin(cognitive_planner_node)\n    cognitive_planner_node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n')),(0,o.yg)("h3",{id:"124-translating-llm-output-to-ros-2-actions"},"12.4 Translating LLM Output to ROS 2 Actions"),(0,o.yg)("p",null,"The ",(0,o.yg)("inlineCode",{parentName:"p"},"action_sequence")," generated by the LLM is a structured representation of the robot's plan. A separate ",(0,o.yg)("strong",{parentName:"p"},"Action Executor")," node will be responsible for:"),(0,o.yg)("ol",null,(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("strong",{parentName:"li"},"Parsing the JSON"),": Decoding the LLM's action sequence."),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("strong",{parentName:"li"},"Mapping to ROS 2 Primitives"),": Calling the appropriate ROS 2 action clients, service clients, or publishing to topics based on the parsed action definitions."),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("strong",{parentName:"li"},"Monitoring Execution"),": Tracking the status of each ROS 2 action (e.g., navigation goal status, service call success)."),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("strong",{parentName:"li"},"Error Handling"),": If an action fails, the executor might trigger a recovery behavior or report back to the cognitive planner (potentially leading to a re-plan).")),(0,o.yg)("p",null,"This layered approach, from voice command to transcribed text, to an LLM-generated action sequence, and finally to ROS 2 execution, allows for highly flexible and intelligent robot control."))}I.isMDXComponent=!0},8711:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/VLA-be71fb9ed37f192cb4aa54fdc0eab000.jpg"},9159:(e,n,t)=>{t.d(n,{A:()=>i});const i="data:image/svg+xml;base64,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"},9960:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/OPEN_VLA-f2ac32d676f7b87d28e614fdef3b574d.jpg"}}]);